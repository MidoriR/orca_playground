{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for killer whale sound classification\n",
    "\n",
    "by Erika Pel√°ez\n",
    "\n",
    "This is a simple Random Forest Classifier trained with the data from the Ford Osborne tape analysis and the dataset built by the get_db.py file by scraping the [Watkins Marine Mammal Sound database](https://cis.whoi.edu/science/B/whalesounds/index.cfm).\n",
    "\n",
    "The non killer examples are made of the noises from other not orca whales, to be specific I took the Humpback Whale (*Megaptera novaeangliae*) and the False Killer Whale (*Pseudorca crassidens*) from the Watkins' data base. For the killer examples I used the Killer Whale (Orcinus orca) from Watkins' and the data from the Ford Osborne tape analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of killer whale files: 121\n",
      "Number of non killer whale files: 746\n"
     ]
    }
   ],
   "source": [
    "killer_files = os.listdir('killer')\n",
    "nonkiller_files = os.listdir('nonkiller')\n",
    "print(f\"Number of killer whale files: {len(killer_files)}\")\n",
    "print(f\"Number of non killer whale files: {len(nonkiller_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n",
    "\n",
    "The files were cut into a maximum of five second chunks with the `cut_sounds.sh` script, however the original files with greater lenght remain in the directory We will read all the files that last less than 6 seconds (to avoid havind the same signal twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rates = []\n",
    "sound_signals = []\n",
    "labels = []\n",
    "base_dirs = {'killer': killer_files, 'nonkiller': nonkiller_files}\n",
    "\n",
    "for folder in base_dirs:\n",
    "    for file in base_dirs[folder]:\n",
    "        if librosa.get_duration(filename=f'{folder}/{file}') < 6:\n",
    "            sound, sampling = librosa.load(f'{folder}/{file}', duration=5.0)\n",
    "            sampling_rates.append(sampling)\n",
    "            sound_signals.append(sound)\n",
    "            labels.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of signals: 827\n",
      "Number of killer whale signals: 119\n",
      "Number of killer whale signals: 708\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of signals: {len(sound_signals)}\")\n",
    "print(f\"Number of killer whale signals: {labels.count('killer')}\")\n",
    "print(f\"Number of killer whale signals: {labels.count('nonkiller')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got a bit less files which is what we wanted. Now we have to pad the files that last less than 5 seconds just so that all of them have te same lenght to make the dataset uniform as we need. We will do a center padding, meaning the signal will be surounded by zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([sound.shape for sound in sound_signals])[0]\n",
    "sound_signals = [librosa.util.pad_center(sound, max_len) for sound in sound_signals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction with mel spectrogam\n",
    "\n",
    "Mel scale is a perceptual scale of pitches judged to be equal in distance to one another by listeners. It has been proven really useful in the voice recognition field, so we will use this representation as our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = np.array([librosa.feature.melspectrogram(y=sound) for sound in sound_signals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a little context of what we did here we need to explain some concepts. The first one is a **spectrum**. A spectrum is just the Fourier transformation of the original signal. Then a **Spectrogram** is a vector representation of the spectrum. You divide the spectrum in different windows of time and for each window size you quantizise the amplitude and turn that into a number between 0-255 the number is directly related to the amplitude of the peak in each window; each vector is layed one beside the other thus creating the hallmark appeareance of the spectrogram.\n",
    "\n",
    "Now The Mel-frequency analysis of speech is based on human experiments. It is observed that human ears act as filters that favor the low frequency region more than the high frequency. Thus making it non-uniformly spaced. Based on that we can now set up a cluster of filters that will only allow to pass certain signals that fall within the filter bounds. As we want to mimic tha human ear, we set up more filters in the low frequency region that the high frequency one. The filters will fall into the mel-scale.\n",
    "\n",
    "Finally, if we filter out our spectrum with Mel-filters we get a Mel-spectrum. So the result of the above operation are actually matrices with spectrums. However the algorithm that we use here takes a vector as input instead of a matrix, that is why we need to *flatten* the matrix by concatenating each row into a single row, thus making it possible for our Random Forest to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sounds, width, height = spectrograms.shape\n",
    "x = spectrograms.reshape(n_sounds, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare our labels to a numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1 if label == 'killer' else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest\n",
    "\n",
    "Random Forest are one Ensemble technique in Machine Learning. They are a group of decision trees that collectively decide the class of each sample. The decision of every tree is computed and then the mode (most frequent) class is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50) \n",
    "scores = cross_val_score(clf, x, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9927710843373493 +/- 0.024573588017314622\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy is: {scores.mean()} +/- {scores.std() * 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is great so we will serialize this model to use later to help us label future incoming data. We use pickle for the serialization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'rf_orca.pkl'\n",
    "\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to reload our model and use it later we can use the following code:\n",
    "\n",
    "```python\n",
    "with  open('rf_orca.pkl', 'rb') as model:\n",
    "    clf = pickle.load(model)\n",
    " \n",
    "clf.predict(x) # will output class label {0, 1}\n",
    "clf.predict_proba(x) # will output prob for each class \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with  open('rf_orca.pkl', 'rb') as model:\n",
    "    clf = pickle.load(model)\n",
    "\n",
    "#clf.predict(x) # will output class label {0, 1}\n",
    "#clf.predict_proba(x) # will output prob for each class \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
